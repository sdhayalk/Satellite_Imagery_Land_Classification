# Enter your network definition here.
# Use Shift+Enter to update the visualization.
name: "ResNet"

layer {
  name: "geodata"
  type: "HDF5Data"
  top: "data"
  top: "label"
  hdf5_data_param {
    source: "G:/DL/satellite_imagery_land_classification/Satellite_Imagery_Land_Classification/dataset_train.txt"
    batch_size: 4
  }
  include {
		phase: TRAIN
	}
}

layer {
  name: "geodata"
  type: "HDF5Data"
  top: "data"
  top: "label"
  hdf5_data_param {
    source: "G:/DL/satellite_imagery_land_classification/Satellite_Imagery_Land_Classification/dataset_validation.txt"
    batch_size: 1
  }
  include {
		phase: TEST
	}
}


# residual block 1 start

layer {
	name: "rb1_bn1"
	type: "BatchNorm"
	bottom: "data"
	top: "rb1_bn1"
	batch_norm_param {
		use_global_stats: false
	}
	include {
		phase: TRAIN
	}
}
layer {
	name: "rb1_bn1"
	type: "BatchNorm"
	bottom: "data"
	top: "rb1_bn1"
	batch_norm_param {
		use_global_stats: true
	}
	include {
		phase: TEST
	}
}

layer {
  name: "rb1_relu1"
  type: "ReLU"
  bottom: "rb1_bn1"
  top: "rb1_relu1"
}

layer {
  	name: "rb1_conv1"
	type: "Convolution"
	bottom: "rb1_relu1"
	top: "rb1_conv1"
	param {
		lr_mult: 1
	}
	param {
		lr_mult: 2
	}
	convolution_param {
		num_output: 64
		kernel_size: 3
		stride: 1
		pad: 1
		weight_filler {
			type: "xavier"
		}
		bias_filler {
			type: "constant"
		}
	}
}

layer {
	name: "rb1_bn2"
	type: "BatchNorm"
	bottom: "rb1_conv1"
	top: "rb1_bn2"
	batch_norm_param {
		use_global_stats: false
	}
	include {
		phase: TRAIN
	}
}
layer {
	name: "rb1_bn2"
	type: "BatchNorm"
	bottom: "rb1_conv1"
	top: "rb1_bn2"
	batch_norm_param {
		use_global_stats: true
	}
	include {
		phase: TEST
	}
}

layer {
  name: "rb1_relu2"
  type: "ReLU"
  bottom: "rb1_bn2"
  top: "rb1_relu2"
}

layer {
  	name: "rb1_conv2"
	type: "Convolution"
	bottom: "rb1_relu2"
	top: "rb1_conv2"
	param {
		lr_mult: 1
	}
	param {
		lr_mult: 2
	}
	convolution_param {
		num_output: 64
		kernel_size: 3
		stride: 1
		pad: 1
		weight_filler {
			type: "xavier"
		}
		bias_filler {
			type: "constant"
		}
	}
}

# 1x1 pointwise conv for inreasing number of channels
layer {
  	name: "rb1_conv0"
	type: "Convolution"
	bottom: "data"
	top: "rb1_conv0"
	param {
		lr_mult: 1
	}
	param {
		lr_mult: 2
	}
	convolution_param {
		num_output: 64
		kernel_size: 1
		stride: 1
		weight_filler {
			type: "xavier"
		}
		bias_filler {
			type: "constant"
		}
	}
}

layer {
	name: "eltwise_sum1"
	type: "Eltwise"
	bottom: "rb1_conv0"
	bottom: "rb1_conv2"
	top: "eltwise_sum1"
	eltwise_param { 
		operation: SUM
	}
}

# residual block 1 end

layer {
  name: "pool1"
  type: "Pooling"
  bottom: "eltwise_sum1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}

# residual block 2 start

layer {
	name: "rb2_bn1"
	type: "BatchNorm"
	bottom: "pool1"
	top: "rb2_bn1"
	batch_norm_param {
		use_global_stats: false
	}
	include {
		phase: TRAIN
	}
}
layer {
	name: "rb2_bn1"
	type: "BatchNorm"
	bottom: "pool1"
	top: "rb2_bn1"
	batch_norm_param {
		use_global_stats: true
	}
	include {
		phase: TEST
	}
}

layer {
  name: "rb2_relu1"
  type: "ReLU"
  bottom: "rb2_bn1"
  top: "rb2_relu1"
}

layer {
  	name: "rb2_conv1"
	type: "Convolution"
	bottom: "rb2_relu1"
	top: "rb2_conv1"
	param {
		lr_mult: 1
	}
	param {
		lr_mult: 2
	}
	convolution_param {
		num_output: 64
		kernel_size: 3
		stride: 1
		pad: 1
		weight_filler {
			type: "xavier"
		}
		bias_filler {
			type: "constant"
		}
	}
}

layer {
	name: "rb2_bn2"
	type: "BatchNorm"
	bottom: "rb2_conv1"
	top: "rb2_bn2"
	batch_norm_param {
		use_global_stats: false
	}
	include {
		phase: TRAIN
	}
}
layer {
	name: "rb2_bn2"
	type: "BatchNorm"
	bottom: "rb2_conv1"
	top: "rb2_bn2"
	batch_norm_param {
		use_global_stats: true
	}
	include {
		phase: TEST
	}
}

layer {
  name: "rb2_relu2"
  type: "ReLU"
  bottom: "rb2_bn2"
  top: "rb2_relu2"
}

layer {
  	name: "rb2_conv2"
	type: "Convolution"
	bottom: "rb2_relu2"
	top: "rb2_conv2"
	param {
		lr_mult: 1
	}
	param {
		lr_mult: 2
	}
	convolution_param {
		num_output: 64
		kernel_size: 3
		stride: 1
		pad: 1
		weight_filler {
			type: "xavier"
		}
		bias_filler {
			type: "constant"
		}
	}
}


layer {
	name: "eltwise_sum2"
	type: "Eltwise"
	bottom: "pool1"
	bottom: "rb2_conv2"
	top: "eltwise_sum2"
	eltwise_param { 
		operation: SUM
	}
}

# residual block 2 end

layer {
  name: "pool2"
  type: "Pooling"
  bottom: "eltwise_sum2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}

# residual block 3 start

layer {
	name: "rb3_bn1"
	type: "BatchNorm"
	bottom: "pool2"
	top: "rb3_bn1"
	batch_norm_param {
		use_global_stats: false
	}
	include {
		phase: TRAIN
	}
}
layer {
	name: "rb3_bn1"
	type: "BatchNorm"
	bottom: "pool2"
	top: "rb3_bn1"
	batch_norm_param {
		use_global_stats: true
	}
	include {
		phase: TEST
	}
}

layer {
  name: "rb3_relu1"
  type: "ReLU"
  bottom: "rb3_bn1"
  top: "rb3_relu1"
}

layer {
  	name: "rb3_conv1"
	type: "Convolution"
	bottom: "rb3_relu1"
	top: "rb3_conv1"
	param {
		lr_mult: 1
	}
	param {
		lr_mult: 2
	}
	convolution_param {
		num_output: 128
		kernel_size: 3
		stride: 1
		pad: 1
		weight_filler {
			type: "xavier"
		}
		bias_filler {
			type: "constant"
		}
	}
}

layer {
	name: "rb3_bn2"
	type: "BatchNorm"
	bottom: "rb3_conv1"
	top: "rb3_bn2"
	batch_norm_param {
		use_global_stats: false
	}
	include {
		phase: TRAIN
	}
}
layer {
	name: "rb3_bn2"
	type: "BatchNorm"
	bottom: "rb3_conv1"
	top: "rb3_bn2"
	batch_norm_param {
		use_global_stats: true
	}
	include {
		phase: TEST
	}
}

layer {
  name: "rb3_relu2"
  type: "ReLU"
  bottom: "rb3_bn2"
  top: "rb3_relu2"
}

layer {
  	name: "rb3_conv2"
	type: "Convolution"
	bottom: "rb3_relu2"
	top: "rb3_conv2"
	param {
		lr_mult: 1
	}
	param {
		lr_mult: 2
	}
	convolution_param {
		num_output: 128
		kernel_size: 3
		stride: 1
		pad: 1
		weight_filler {
			type: "xavier"
		}
		bias_filler {
			type: "constant"
		}
	}
}

# 1x1 pointwise conv for inreasing number of channels
layer {
  	name: "rb3_conv0"
	type: "Convolution"
	bottom: "pool2"
	top: "rb3_conv0"
	param {
		lr_mult: 1
	}
	param {
		lr_mult: 2
	}
	convolution_param {
		num_output: 128
		kernel_size: 1
		stride: 1
		weight_filler {
			type: "xavier"
		}
		bias_filler {
			type: "constant"
		}
	}
}

layer {
	name: "eltwise_sum3"
	type: "Eltwise"
	bottom: "rb3_conv0"
	bottom: "rb3_conv2"
	top: "eltwise_sum3"
	eltwise_param { 
		operation: SUM
	}
}

# residual block 3 end

layer {
  name: "pool3"
  type: "Pooling"
  bottom: "eltwise_sum3"
  top: "pool3"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}


# residual block 4 start

layer {
	name: "rb4_bn1"
	type: "BatchNorm"
	bottom: "pool3"
	top: "rb4_bn1"
	batch_norm_param {
		use_global_stats: false
	}
	include {
		phase: TRAIN
	}
}
layer {
	name: "rb4_bn1"
	type: "BatchNorm"
	bottom: "pool3"
	top: "rb4_bn1"
	batch_norm_param {
		use_global_stats: true
	}
	include {
		phase: TEST
	}
}

layer {
  name: "rb4_relu1"
  type: "ReLU"
  bottom: "rb4_bn1"
  top: "rb4_relu1"
}

layer {
  	name: "rb4_conv1"
	type: "Convolution"
	bottom: "rb4_relu1"
	top: "rb4_conv1"
	param {
		lr_mult: 1
	}
	param {
		lr_mult: 2
	}
	convolution_param {
		num_output: 128
		kernel_size: 3
		stride: 1
		pad: 1
		weight_filler {
			type: "xavier"
		}
		bias_filler {
			type: "constant"
		}
	}
}

layer {
	name: "rb4_bn2"
	type: "BatchNorm"
	bottom: "rb4_conv1"
	top: "rb4_bn2"
	batch_norm_param {
		use_global_stats: false
	}
	include {
		phase: TRAIN
	}
}
layer {
	name: "rb4_bn2"
	type: "BatchNorm"
	bottom: "rb4_conv1"
	top: "rb4_bn2"
	batch_norm_param {
		use_global_stats: true
	}
	include {
		phase: TEST
	}
}

layer {
  name: "rb4_relu2"
  type: "ReLU"
  bottom: "rb4_bn2"
  top: "rb4_relu2"
}

layer {
  	name: "rb4_conv2"
	type: "Convolution"
	bottom: "rb4_relu2"
	top: "rb4_conv2"
	param {
		lr_mult: 1
	}
	param {
		lr_mult: 2
	}
	convolution_param {
		num_output: 128
		kernel_size: 3
		stride: 1
		pad: 1
		weight_filler {
			type: "xavier"
		}
		bias_filler {
			type: "constant"
		}
	}
}


layer {
	name: "eltwise_sum4"
	type: "Eltwise"
	bottom: "pool3"
	bottom: "rb4_conv2"
	top: "eltwise_sum4"
	eltwise_param { 
		operation: SUM
	}
}

# residual block 4 end

layer {
  name: "pool4"
  type: "Pooling"
  bottom: "eltwise_sum4"
  top: "pool4"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}

# residual block 5 start

layer {
	name: "rb5_bn1"
	type: "BatchNorm"
	bottom: "pool4"
	top: "rb5_bn1"
	batch_norm_param {
		use_global_stats: false
	}
	include {
		phase: TRAIN
	}
}
layer {
	name: "rb5_bn1"
	type: "BatchNorm"
	bottom: "pool4"
	top: "rb5_bn1"
	batch_norm_param {
		use_global_stats: true
	}
	include {
		phase: TEST
	}
}

layer {
  name: "rb5_relu1"
  type: "ReLU"
  bottom: "rb5_bn1"
  top: "rb5_relu1"
}

layer {
  	name: "rb5_conv1"
	type: "Convolution"
	bottom: "rb5_relu1"
	top: "rb5_conv1"
	param {
		lr_mult: 1
	}
	param {
		lr_mult: 2
	}
	convolution_param {
		num_output: 256
		kernel_size: 3
		stride: 1
		pad: 1
		weight_filler {
			type: "xavier"
		}
		bias_filler {
			type: "constant"
		}
	}
}

layer {
	name: "rb5_bn2"
	type: "BatchNorm"
	bottom: "rb5_conv1"
	top: "rb5_bn2"
	batch_norm_param {
		use_global_stats: false
	}
	include {
		phase: TRAIN
	}
}
layer {
	name: "rb5_bn2"
	type: "BatchNorm"
	bottom: "rb5_conv1"
	top: "rb5_bn2"
	batch_norm_param {
		use_global_stats: true
	}
	include {
		phase: TEST
	}
}

layer {
  name: "rb5_relu2"
  type: "ReLU"
  bottom: "rb5_bn2"
  top: "rb5_relu2"
}

layer {
  	name: "rb5_conv2"
	type: "Convolution"
	bottom: "rb5_relu2"
	top: "rb5_conv2"
	param {
		lr_mult: 1
	}
	param {
		lr_mult: 2
	}
	convolution_param {
		num_output: 256
		kernel_size: 3
		stride: 1
		pad: 1
		weight_filler {
			type: "xavier"
		}
		bias_filler {
			type: "constant"
		}
	}
}

# 1x1 pointwise conv for inreasing number of channels
layer {
  	name: "rb5_conv0"
	type: "Convolution"
	bottom: "pool4"
	top: "rb5_conv0"
	param {
		lr_mult: 1
	}
	param {
		lr_mult: 2
	}
	convolution_param {
		num_output: 256
		kernel_size: 1
		stride: 1
		weight_filler {
			type: "xavier"
		}
		bias_filler {
			type: "constant"
		}
	}
}

layer {
	name: "eltwise_sum5"
	type: "Eltwise"
	bottom: "rb5_conv0"
	bottom: "rb5_conv2"
	top: "eltwise_sum5"
	eltwise_param { 
		operation: SUM
	}
}

# residual block 5 end

layer {
  name: "pool5"
  type: "Pooling"
  bottom: "eltwise_sum5"
  top: "pool5"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}

# residual block 6 start

layer {
	name: "rb6_bn1"
	type: "BatchNorm"
	bottom: "pool5"
	top: "rb6_bn1"
	batch_norm_param {
		use_global_stats: false
	}
	include {
		phase: TRAIN
	}
}
layer {
	name: "rb6_bn1"
	type: "BatchNorm"
	bottom: "pool5"
	top: "rb6_bn1"
	batch_norm_param {
		use_global_stats: true
	}
	include {
		phase: TEST
	}
}

layer {
  name: "rb6_relu1"
  type: "ReLU"
  bottom: "rb6_bn1"
  top: "rb6_relu1"
}

layer {
  	name: "rb6_conv1"
	type: "Convolution"
	bottom: "rb6_relu1"
	top: "rb6_conv1"
	param {
		lr_mult: 1
	}
	param {
		lr_mult: 2
	}
	convolution_param {
		num_output: 256
		kernel_size: 3
		stride: 1
		pad: 1
		weight_filler {
			type: "xavier"
		}
		bias_filler {
			type: "constant"
		}
	}
}

layer {
	name: "rb6_bn2"
	type: "BatchNorm"
	bottom: "rb6_conv1"
	top: "rb6_bn2"
	batch_norm_param {
		use_global_stats: false
	}
	include {
		phase: TRAIN
	}
}
layer {
	name: "rb6_bn2"
	type: "BatchNorm"
	bottom: "rb6_conv1"
	top: "rb6_bn2"
	batch_norm_param {
		use_global_stats: true
	}
	include {
		phase: TEST
	}
}

layer {
  name: "rb6_relu2"
  type: "ReLU"
  bottom: "rb6_bn2"
  top: "rb6_relu2"
}

layer {
  	name: "rb6_conv2"
	type: "Convolution"
	bottom: "rb6_relu2"
	top: "rb6_conv2"
	param {
		lr_mult: 1
	}
	param {
		lr_mult: 2
	}
	convolution_param {
		num_output: 256
		kernel_size: 3
		stride: 1
		pad: 1
		weight_filler {
			type: "xavier"
		}
		bias_filler {
			type: "constant"
		}
	}
}


layer {
	name: "eltwise_sum6"
	type: "Eltwise"
	bottom: "pool5"
	bottom: "rb6_conv2"
	top: "eltwise_sum6"
	eltwise_param { 
		operation: SUM
	}
}

# residual block 6 end

layer {
  name: "pool6"
  type: "Pooling"
  bottom: "eltwise_sum6"
  top: "pool6"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}




# defining first fully connected layer, also called as inner product, with name ip1
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool6"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}

# defining second fully connected layer, also called as inner product, with name ip2
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 21
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}

layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip2"
  bottom: "label"
  top: "loss"
}

layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "ip2"
  bottom: "label"
  top: "accuracy"
}
